{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rajesh24mcs115/24mcs115-Exp7-RajesKumarPal/blob/main/Experiment_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download required corpora and tokenization resources\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')  # Requested modification\n",
        "\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# Combine all texts in the Gutenberg corpus into a single large string\n",
        "texts = [gutenberg.raw(fileid) for fileid in gutenberg.fileids()]\n",
        "text = \"\\n\".join(texts).lower()  # Convert to lowercase\n",
        "print(\"Total length of Gutenberg corpus (characters):\", len(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56OF7uMaZiwH",
        "outputId": "db6ce51e-c012-44ab-dfc8-1033ddf990f3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total length of Gutenberg corpus (characters): 11793335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-5COj93XPt1",
        "outputId": "8bb76dae-e92a-44af-be35-e490b1d05b92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens: 2539731\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the text (punctuation is preserved as tokens)\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(\"Total tokens:\", len(tokens))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Define the vocabulary size (around 300 tokens as required)\n",
        "vocab_size = 300\n",
        "\n",
        "# Count token frequencies and select the top vocab_size tokens\n",
        "counter = Counter(tokens)\n",
        "most_common = counter.most_common(vocab_size)\n",
        "vocab = [word for word, count in most_common]\n",
        "\n",
        "# Create mapping dictionaries for word-to-index and index-to-word\n",
        "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "\n",
        "print(\"Vocabulary Size:\", len(vocab))\n",
        "print(\"Sample vocabulary:\", vocab[:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dKub-MUXcJX",
        "outputId": "94f9cb00-df6d-4c45-f57d-987d11ee26c1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 300\n",
            "Sample vocabulary: [',', 'the', 'and', '.', 'of', 'to', 'a', 'in', 'i', 'that', ';', 'he', 'it', 'his', 'for', 'was', 'not', 'with', \"''\", 'is']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract all 4‑grams (each 4‑gram is a sequence of 4 adjacent tokens)\n",
        "# Only include 4‑grams where every token is in our vocabulary.\n",
        "fourgrams = []\n",
        "for i in range(len(tokens) - 3):\n",
        "    gram = tokens[i:i+4]\n",
        "    if all(word in vocab for word in gram):\n",
        "        fourgrams.append(gram)\n",
        "\n",
        "print(\"Total 4-grams extracted:\", len(fourgrams))\n",
        "total_required = 400000 + 50000 + 50000  # Target: 500K total 4-grams\n",
        "if len(fourgrams) < total_required:\n",
        "    print(\"Warning: Not enough 4-grams available. The available data will be used for splitting.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQSnulaxXgui",
        "outputId": "d1ea369b-e193-4f53-f1f3-17d224b773ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total 4-grams extracted: 563786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# For each 4‑gram, the first three tokens are the input and the fourth token is the target (label)\n",
        "inputs = []\n",
        "labels = []\n",
        "for gram in fourgrams:\n",
        "    input_seq = [word2idx[word] for word in gram[:3]]\n",
        "    label = word2idx[gram[3]]\n",
        "    inputs.append(input_seq)\n",
        "    labels.append(label)\n",
        "\n",
        "inputs = np.array(inputs)\n",
        "labels = np.array(labels)\n",
        "print(\"Input shape:\", inputs.shape)\n",
        "print(\"Labels shape:\", labels.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqjJPef3asrh",
        "outputId": "633d3040-0830-4811-c775-f2df8ced67d2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (563786, 3)\n",
            "Labels shape: (563786,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "num_samples = len(inputs)\n",
        "indices = list(range(num_samples))\n",
        "random.shuffle(indices)\n",
        "\n",
        "train_end = int(0.8 * num_samples)\n",
        "val_end = int(0.9 * num_samples)\n",
        "\n",
        "X_train = inputs[indices[:train_end]]\n",
        "y_train = labels[indices[:train_end]]\n",
        "X_val = inputs[indices[train_end:val_end]]\n",
        "y_val = labels[indices[train_end:val_end]]\n",
        "X_test = inputs[indices[val_end:]]\n",
        "y_test = labels[indices[val_end:]]\n",
        "\n",
        "print(\"Training samples:\", len(X_train))\n",
        "print(\"Validation samples:\", len(X_val))\n",
        "print(\"Test samples:\", len(X_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56KpK3G1gQ0x",
        "outputId": "317c80bd-7623-44fd-ea76-0d8c5f44f87b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 451028\n",
            "Validation samples: 56379\n",
            "Test samples: 56379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Flatten, Dropout, Input\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization\n",
        "\n",
        "embedding_dim = 50  # Embedding dimension\n",
        "\n",
        "# ------------------ RNN Model (LSTM) ------------------\n",
        "# Removed the deprecated `input_length` argument.\n",
        "rnn_model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, name=\"rnn_embedding\"),\n",
        "    LSTM(128, return_sequences=False),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "# Explicitly build the model with input shape (None, 3) to initialize parameters.\n",
        "rnn_model.build(input_shape=(None, 3))\n",
        "rnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "rnn_model.summary()\n",
        "\n",
        "# ------------------ Transformer Model ------------------\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = Sequential([Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "input_layer = Input(shape=(3,))\n",
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim, name=\"transformer_embedding\")(input_layer)\n",
        "transformer_block = TransformerBlock(embed_dim=embedding_dim, num_heads=4, ff_dim=128)(embedding_layer)\n",
        "flatten = Flatten()(transformer_block)\n",
        "output_layer = Dense(vocab_size, activation=\"softmax\")(flatten)\n",
        "\n",
        "transformer_model = Model(inputs=input_layer, outputs=output_layer)\n",
        "transformer_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "transformer_model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "_OKldGPXgWc8",
        "outputId": "2ac35628-7a78-4935-97f0-a5b1fb4b343f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ rnn_embedding (\u001b[38;5;33mEmbedding\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m50\u001b[0m)               │          \u001b[38;5;34m15,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m91,648\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m16,512\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)                 │          \u001b[38;5;34m38,700\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ rnn_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">15,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">91,648</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">38,700</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m161,860\u001b[0m (632.27 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">161,860</span> (632.27 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m161,860\u001b[0m (632.27 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">161,860</span> (632.27 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_embedding (\u001b[38;5;33mEmbedding\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m50\u001b[0m)               │          \u001b[38;5;34m15,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block (\u001b[38;5;33mTransformerBlock\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m50\u001b[0m)               │          \u001b[38;5;34m53,828\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)                 │          \u001b[38;5;34m45,300\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">15,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_block (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)               │          <span style=\"color: #00af00; text-decoration-color: #00af00\">53,828</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">45,300</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m114,128\u001b[0m (445.81 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">114,128</span> (445.81 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m114,128\u001b[0m (445.81 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">114,128</span> (445.81 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10      # Adjust epochs as needed\n",
        "batch_size = 128 # Batch size for training\n",
        "\n",
        "print(\"\\nTraining RNN Model...\")\n",
        "rnn_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n",
        "\n",
        "print(\"\\nTraining Transformer Model...\")\n",
        "transformer_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdz77rdagY_2",
        "outputId": "c0642c32-8eb7-423d-beb6-edd65bab4ef2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training RNN Model...\n",
            "Epoch 1/10\n",
            "\u001b[1m3524/3524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 12ms/step - accuracy: 0.1619 - loss: 4.2255 - val_accuracy: 0.2481 - val_loss: 3.4526\n",
            "Epoch 2/10\n",
            "\u001b[1m3524/3524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 12ms/step - accuracy: 0.2552 - loss: 3.3847 - val_accuracy: 0.2645 - val_loss: 3.3075\n",
            "Epoch 3/10\n",
            "\u001b[1m3524/3524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 12ms/step - accuracy: 0.2696 - loss: 3.2518 - val_accuracy: 0.2743 - val_loss: 3.2387\n",
            "Epoch 4/10\n",
            "\u001b[1m3524/3524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 12ms/step - accuracy: 0.2805 - loss: 3.1753 - val_accuracy: 0.2811 - val_loss: 3.1964\n",
            "Epoch 5/10\n",
            "\u001b[1m3524/3524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 12ms/step - accuracy: 0.2857 - loss: 3.1273 - val_accuracy: 0.2843 - val_loss: 3.1725\n",
            "Epoch 6/10\n",
            "\u001b[1m3524/3524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 12ms/step - accuracy: 0.2909 - loss: 3.0808 - val_accuracy: 0.2868 - val_loss: 3.1572\n",
            "Epoch 7/10\n",
            "\u001b[1m3524/3524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 13ms/step - accuracy: 0.2970 - loss: 3.0494 - val_accuracy: 0.2873 - val_loss: 3.1507\n",
            "Epoch 8/10\n",
            "\u001b[1m3524/3524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 13ms/step - accuracy: 0.2988 - loss: 3.0237 - val_accuracy: 0.2920 - val_loss: 3.1361\n",
            "Epoch 9/10\n",
            "\u001b[1m3524/3524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 13ms/step - accuracy: 0.3040 - loss: 2.9928 - val_accuracy: 0.2910 - val_loss: 3.1303\n",
            "Epoch 10/10\n",
            "\u001b[1m3524/3524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 13ms/step - accuracy: 0.3068 - loss: 2.9723 - val_accuracy: 0.2913 - val_loss: 3.1295\n",
            "\n",
            "Training Transformer Model...\n",
            "Epoch 1/10\n",
            "\u001b[1m3524/3524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 34ms/step - accuracy: 0.2176 - loss: 3.8323 - val_accuracy: 0.2603 - val_loss: 3.3456\n",
            "Epoch 2/10\n",
            "\u001b[1m3524/3524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 34ms/step - accuracy: 0.2643 - loss: 3.3082 - val_accuracy: 0.2717 - val_loss: 3.2710\n",
            "Epoch 3/10\n",
            "\u001b[1m3524/3524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 36ms/step - accuracy: 0.2712 - loss: 3.2325 - val_accuracy: 0.2740 - val_loss: 3.2418\n",
            "Epoch 4/10\n",
            "\u001b[1m3524/3524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 35ms/step - accuracy: 0.2753 - loss: 3.1910 - val_accuracy: 0.2768 - val_loss: 3.2251\n",
            "Epoch 5/10\n",
            "\u001b[1m3524/3524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 34ms/step - accuracy: 0.2788 - loss: 3.1682 - val_accuracy: 0.2779 - val_loss: 3.2104\n",
            "Epoch 6/10\n",
            "\u001b[1m3524/3524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 35ms/step - accuracy: 0.2794 - loss: 3.1558 - val_accuracy: 0.2787 - val_loss: 3.2008\n",
            "Epoch 7/10\n",
            "\u001b[1m3524/3524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 34ms/step - accuracy: 0.2835 - loss: 3.1326 - val_accuracy: 0.2791 - val_loss: 3.1986\n",
            "Epoch 8/10\n",
            "\u001b[1m3524/3524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 34ms/step - accuracy: 0.2830 - loss: 3.1274 - val_accuracy: 0.2816 - val_loss: 3.1852\n",
            "Epoch 9/10\n",
            "\u001b[1m3524/3524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 34ms/step - accuracy: 0.2844 - loss: 3.1159 - val_accuracy: 0.2818 - val_loss: 3.1784\n",
            "Epoch 10/10\n",
            "\u001b[1m3524/3524\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 34ms/step - accuracy: 0.2860 - loss: 3.1023 - val_accuracy: 0.2829 - val_loss: 3.1737\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x796a2014dbd0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate RNN Model on test set\n",
        "rnn_loss, rnn_acc = rnn_model.evaluate(X_test, y_test)\n",
        "print(\"RNN Model - Test Loss:\", rnn_loss, \"Test Accuracy:\", rnn_acc)\n",
        "\n",
        "# Evaluate Transformer Model on test set\n",
        "transformer_loss, transformer_acc = transformer_model.evaluate(X_test, y_test)\n",
        "print(\"Transformer Model - Test Loss:\", transformer_loss, \"Test Accuracy:\", transformer_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHc4Pbvmg2WD",
        "outputId": "947430f7-7018-43ff-ac2f-89b4eac4cf01"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1762/1762\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.2880 - loss: 3.1390\n",
            "RNN Model - Test Loss: 3.1303884983062744 Test Accuracy: 0.28923889994621277\n",
            "\u001b[1m1762/1762\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.2819 - loss: 3.1868\n",
            "Transformer Model - Test Loss: 3.1814510822296143 Test Accuracy: 0.2809556722640991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_word(model, word_sequence):\n",
        "    # Convert words to indices; default to index 0 if a word is not found\n",
        "    seq = [word2idx.get(word, 0) for word in word_sequence]\n",
        "    seq = np.array(seq).reshape(1, -1)\n",
        "    pred_probs = model.predict(seq)\n",
        "    predicted_index = np.argmax(pred_probs, axis=1)[0]\n",
        "    return idx2word[predicted_index]\n",
        "\n",
        "# Example sequences for next‑word prediction (more common sequences added):\n",
        "sequences = [\n",
        "    [\"government\", \"of\", \"united\"],\n",
        "    [\"city\", \"of\", \"new\"],\n",
        "    [\"life\", \"in\", \"the\"],\n",
        "    [\"he\", \"is\", \"the\"],\n",
        "    [\"at\", \"the\", \"end\"],\n",
        "    [\"in\", \"the\", \"middle\"],\n",
        "    [\"this\", \"is\", \"a\"],\n",
        "    [\"one\", \"of\", \"the\"],\n",
        "    [\"it\", \"was\", \"a\"]\n",
        "]\n",
        "\n",
        "print(\"\\nNext-word Predictions:\")\n",
        "for seq in sequences:\n",
        "    next_word_rnn = predict_next_word(rnn_model, seq)\n",
        "    next_word_trans = predict_next_word(transformer_model, seq)\n",
        "    print(f\"Input: {seq}\")\n",
        "    print(f\"  RNN Prediction: {next_word_rnn}\")\n",
        "    print(f\"  Transformer Prediction: {next_word_trans}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ga9BWS9g9x7",
        "outputId": "9889f7d3-7a13-40ba-bce6-ee63ad524a78"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Next-word Predictions:\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Input: ['government', 'of', 'united']\n",
            "  RNN Prediction: as\n",
            "  Transformer Prediction: and\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Input: ['city', 'of', 'new']\n",
            "  RNN Prediction: and\n",
            "  Transformer Prediction: and\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Input: ['life', 'in', 'the']\n",
            "  RNN Prediction: world\n",
            "  Transformer Prediction: world\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Input: ['he', 'is', 'the']\n",
            "  RNN Prediction: very\n",
            "  Transformer Prediction: lord\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "Input: ['at', 'the', 'end']\n",
            "  RNN Prediction: of\n",
            "  Transformer Prediction: of\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Input: ['in', 'the', 'middle']\n",
            "  RNN Prediction: and\n",
            "  Transformer Prediction: and\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Input: ['this', 'is', 'a']\n",
            "  RNN Prediction: very\n",
            "  Transformer Prediction: man\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Input: ['one', 'of', 'the']\n",
            "  RNN Prediction: people\n",
            "  Transformer Prediction: people\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Input: ['it', 'was', 'a']\n",
            "  RNN Prediction: very\n",
            "  Transformer Prediction: very\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract learned embeddings from both models\n",
        "rnn_embeddings = rnn_model.get_layer(\"rnn_embedding\").get_weights()[0]\n",
        "transformer_embeddings = transformer_model.get_layer(\"transformer_embedding\").get_weights()[0]\n",
        "\n",
        "def cosine_similarity(vec1, vec2, epsilon=1e-10):\n",
        "  return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2) + epsilon)\n",
        "\n",
        "def find_nearest_words(target_word, embeddings, word2idx, idx2word, top_n=5):\n",
        "    if target_word not in word2idx:\n",
        "        return f\"Word '{target_word}' not in vocabulary.\"\n",
        "    target_vec = embeddings[word2idx[target_word]]\n",
        "    similarities = [(idx2word[idx], cosine_similarity(target_vec, embeddings[idx]))\n",
        "                    for idx in range(len(embeddings))]\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    return similarities[1:top_n+1]  # Exclude the target word itself\n",
        "\n",
        "# Using words known to be in the Reuters vocabulary:\n",
        "test_words = [\"day\", \"could\", \"said\", \"for\"]\n",
        "\n",
        "print(\"\\n==== RNN Model Nearest Words ====\")\n",
        "for word in test_words:\n",
        "    print(f\"Nearest words to '{word}' (RNN):\", find_nearest_words(word, rnn_embeddings, word2idx, idx2word))\n",
        "\n",
        "print(\"\\n==== Transformer Model Nearest Words ====\")\n",
        "for word in test_words:\n",
        "    print(f\"Nearest words to '{word}' (Transformer):\", find_nearest_words(word, transformer_embeddings, word2idx, idx2word))\n",
        "\n",
        "def cosine_distance(word1, word2, embeddings, word2idx):\n",
        "    if word1 not in word2idx or word2 not in word2idx:\n",
        "        return f\"One or both words not in vocabulary.\"\n",
        "    vec1 = embeddings[word2idx[word1]]\n",
        "    vec2 = embeddings[word2idx[word2]]\n",
        "    return 1 - cosine_similarity(vec1, vec2)\n",
        "\n",
        "# Example: Cosine distance between 'said' and 'it'\n",
        "distance_rnn = cosine_distance(\"said\", \"it\", rnn_embeddings, word2idx)\n",
        "distance_trans = cosine_distance(\"said\", \"it\", transformer_embeddings, word2idx)\n",
        "print(f\"\\nCosine distance between 'said' and 'it' (RNN): {distance_rnn}\")\n",
        "print(f\"Cosine distance between 'said' and 'it' (Transformer): {distance_trans}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IXoyVyshQtP",
        "outputId": "3246b5b2-eb75-45c6-edae-dbbe36dfc0ab"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== RNN Model Nearest Words ====\n",
            "Nearest words to 'day' (RNN): [('days', np.float32(0.6602367)), ('night', np.float32(0.61013925)), ('side', np.float32(0.5977876)), ('thereof', np.float32(0.510229)), ('city', np.float32(0.48644426))]\n",
            "Nearest words to 'could' (RNN): [('can', np.float32(0.7042334)), ('should', np.float32(0.6752854)), ('must', np.float32(0.5555664)), ('would', np.float32(0.5505268)), ('shall', np.float32(0.5119374))]\n",
            "Nearest words to 'said' (RNN): [('saying', np.float32(0.75261194)), ('say', np.float32(0.70133376)), ('saith', np.float32(0.6533459)), ('cried', np.float32(0.5755853)), ('answered', np.float32(0.57381123))]\n",
            "Nearest words to 'for' (RNN): [('but', np.float32(0.47366503)), ('after', np.float32(0.41693103)), ('upon', np.float32(0.4085696)), ('behold', np.float32(0.39826405)), ('with', np.float32(0.3837102))]\n",
            "\n",
            "==== Transformer Model Nearest Words ====\n",
            "Nearest words to 'day' (Transformer): [('night', np.float32(0.65256286)), ('time', np.float32(0.49492806)), ('days', np.float32(0.4565443)), ('morning', np.float32(0.44795886)), ('fire', np.float32(0.43162182))]\n",
            "Nearest words to 'could' (Transformer): [('can', np.float32(0.7509182)), ('would', np.float32(0.51971424)), ('might', np.float32(0.467414)), ('must', np.float32(0.43663344)), ('should', np.float32(0.41262552))]\n",
            "Nearest words to 'said' (Transformer): [('say', np.float32(0.5912247)), ('saying', np.float32(0.58292466)), ('saith', np.float32(0.5752415)), ('cried', np.float32(0.55875415)), ('answered', np.float32(0.48466906))]\n",
            "Nearest words to 'for' (Transformer): [('after', np.float32(0.4245128)), ('against', np.float32(0.40058738)), ('about', np.float32(0.39662313)), ('like', np.float32(0.36706915)), ('with', np.float32(0.34197077))]\n",
            "\n",
            "Cosine distance between 'said' and 'it' (RNN): 1.0366735458374023\n",
            "Cosine distance between 'said' and 'it' (Transformer): 1.0849541425704956\n"
          ]
        }
      ]
    }
  ]
}